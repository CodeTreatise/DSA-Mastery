# 8.2 Heap Sort Complexity Analysis

## Overview

This document provides a deep dive into the complexity analysis of Heap Sort, including the non-obvious O(n) build heap proof and comparisons with other sorting algorithms.

---

## ‚ö° Time Complexity Summary

| Operation | Time | Details |
|-----------|------|---------|
| Build Heap |" **O(n)** "| NOT O(n log n)! |
| Extract All |" O(n log n) "| n extractions * O(log n) each |
| **Total** |" **O(n log n)** "| Build + Extract |

---

## üìê Build Heap: Why O(n)?

### The Intuitive (but Wrong) Analysis

```
"We call heapify on n/2 nodes, each takes O(log n)..."
Wrong conclusion: O(n/2 * log n) = O(n log n)
```

### The Correct Analysis

**Key insight:** Not all nodes are at the same height!

```
Complete binary tree with n nodes:

Level 0 (root):     1 node,  height = log n
Level 1:            2 nodes, height = log n - 1
Level 2:            4 nodes, height = log n - 2
...
Level h:            2^h nodes, height = log n - h
...
Last level:         ~n/2 nodes, height = 0

Heapify cost = O(height from that node to leaf)
```

### Mathematical Proof

**Total work:**
$$\sum_{h=0}^{\log n} \frac{n}{2^{h+1}} \cdot O(h) = O(n) \sum_{h=0}^{\log n} \frac{h}{2^{h+1}}$$

The series $\sum_{h=0}^{\infty} \frac{h}{2^h}$ converges to a constant (= 2).

Therefore: **Build Heap = O(n)**

### Visual Intuition

```
         Level    Nodes    Heapify Cost    Total Work
           0       1          log n         1 * log n
           1       2          log n - 1     2 * (log n - 1)
           2       4          log n - 2     4 * (log n - 2)
          ...
          h      n/4          1             n/4 * 1
          h+1    n/2          0             n/2 * 0  ‚Üê Half the nodes, 0 work!

Most nodes are at the bottom ‚Üí least work each
Few nodes at top ‚Üí most work each
Net effect: O(n)
```

---

## üìê Extract All: O(n log n)

**Analysis:**
- n extractions
- Each extraction: swap + heapify from root
- Heapify from root: O(log n)
- Total: n * O(log n) = O(n log n)

**Note:** This CANNOT be improved to O(n). Each extraction can potentially traverse the full height.

---

## üíæ Space Complexity

| Implementation | Space | Notes |
|----------------|-------|-------|
| In-place (iterative heapify) |" O(1) "| Optimal |
| Recursive heapify |" O(log n) "| Call stack |
| With auxiliary array |" O(n) "| Not needed |

**Why O(log n) for recursive?**
- Max recursion depth = tree height = log n
- Each call uses constant stack space

---

## üîÑ Comparison with Other Sorts

### Time Complexity

| Algorithm | Best | Average | Worst | Notes |
|-----------|------|---------|-------|-------|
| **Heap Sort** |" O(n log n) "| O(n log n) |" O(n log n) "| Guaranteed |
| Quick Sort |" O(n log n) "| O(n log n) |" O(n¬≤) "| Worst on sorted |
| Merge Sort |" O(n log n) "| O(n log n) |" O(n log n) "| Guaranteed |
| Tim Sort |" O(n) "| O(n log n) |" O(n log n) "| Best on nearly sorted |
| Insertion |" O(n) "| O(n¬≤) |" O(n¬≤) "| Best on nearly sorted |

### Space Complexity

| Algorithm | Space | In-Place? |
|-----------|-------|-----------|
| **Heap Sort** |" O(1) "| ‚úÖ Yes |
| Quick Sort |" O(log n) "| ‚úÖ Yes (stack) |
| Merge Sort |" O(n) "| ‚ùå No |
| Tim Sort |" O(n) "| ‚ùå No |

### Stability

| Algorithm | Stable? | Why/Why Not |
|-----------|---------|-------------|
| **Heap Sort** | ‚ùå No | Swaps can reorder equals |
| Quick Sort | ‚ùå No | Partition can reorder |
| Merge Sort | ‚úÖ Yes | Merge preserves order |
| Tim Sort | ‚úÖ Yes | Based on Merge Sort |

### Cache Performance

```
Heap Sort:   Poor - jumps between parent/child (not contiguous)
Quick Sort:  Good - mostly sequential access
Merge Sort:  Moderate - sequential but needs extra space
```

**Why Quick Sort is Often Faster:**
- Better cache locality (sequential partitioning)
- Fewer comparisons on average
- Simpler inner loop

---

## üìä When to Choose Heap Sort

### ‚úÖ Prefer Heap Sort When:

1. **Guaranteed worst-case needed**
   - Real-time systems
   - Security-critical code (predictable timing)

2. **Memory is extremely limited**
   - Embedded systems
   - O(1) space is required

3. **Partial sorting (Top K)**
   - Only need k largest/smallest
   - Build heap O(n) + k extractions O(k log n)
   - Total: O(n + k log n)

4. **External sorting component**
   - K-way merge uses heaps

### ‚ùå Avoid Heap Sort When:

1. **Stability required**
   - Use Merge Sort or Tim Sort

2. **Nearly sorted data**
   - Use Insertion Sort or Tim Sort

3. **General purpose sorting**
   - Quick Sort is typically faster

4. **Cache performance matters**
   - Quick Sort has better locality

---

## üßÆ Detailed Operation Counts

### Comparisons

| Algorithm | Comparisons (Worst) |
|-----------|---------------------|
| Heap Sort | ~2n log n |
| Merge Sort | ~n log n |
| Quick Sort | ~n¬≤ (worst), ~1.4n log n (avg) |

**Why more comparisons in Heap Sort?**
- Heapify compares parent with BOTH children each step
- 2 comparisons per level traversed

### Swaps

| Algorithm | Swaps (Worst) |
|-----------|---------------|
| Heap Sort | ~n log n |
| Quick Sort | ~n¬≤ (worst), ~n/3 (avg) |
| Merge Sort | 0 (copies instead) |

---

## üíª Benchmark Considerations

```python
import random
import time

def benchmark_sorts(n=10000):
    """Compare sort performance."""
    arr = [random.randint(0, n) for _ in range(n)]
    
    # Heap Sort
    arr1 = arr.copy()
    start = time.time()
    heap_sort(arr1)
    heap_time = time.time() - start
    
    # Python's Tim Sort
    arr2 = arr.copy()
    start = time.time()
    arr2.sort()
    tim_time = time.time() - start
    
    print(f"Heap Sort: {heap_time:.4f}s")
    print(f"Tim Sort:  {tim_time:.4f}s")
    print(f"Ratio: {heap_time/tim_time:.1f}x slower")

# Typical result: Heap Sort 3-5x slower than Tim Sort
# Due to cache performance and Python overhead
```

**Real-world observation:**
- Python's `heapq` + list ‚Üí slower than `sorted()`
- For most practical purposes, use built-in sorts
- Heap Sort value is theoretical understanding + specific use cases

---

## üéØ Key Formulas to Remember

### Build Heap
$$T_{build} = O(n)$$

### Extract All
$$T_{extract} = O(n \log n)$$

### Total Heap Sort
$$T_{total} = O(n) + O(n \log n) = O(n \log n)$$

### Top K Elements
$$T_{topK} = O(n + k \log n)$$

---

## ‚ö†Ô∏è Common Analysis Mistakes

### 1. Build Heap is O(n log n)

```
‚ùå Wrong: "Each heapify is O(log n), we do n/2, so O(n log n)"

‚úÖ Correct: Most nodes are near leaves with height ‚âà 0
            Sum converges to O(n)
```

### 2. Heap Sort Can Be Made Stable

```
‚ùå Wrong: "Just track original indices"
   (Doesn't help - heap operations still reorder)

‚úÖ Correct: Heap Sort is fundamentally unstable
            Use Merge Sort if stability needed
```

### 3. Heap Sort is Always Better Than Quick Sort

```
‚ùå Wrong: "Heap Sort has guaranteed O(n log n), Quick Sort O(n¬≤)"

‚úÖ Correct: Quick Sort is typically 2-3x faster on average
            due to cache locality and fewer comparisons
            (Use Quick Sort with randomization/median-of-3)
```

---

## üìù Practice Questions

1. **Prove Build Heap is O(n)** using the summation formula
2. **Why is Heap Sort not stable?** Give a counter-example
3. **When would you choose Heap Sort over Quick Sort?**
4. **What's the space complexity of recursive vs iterative Heap Sort?**

<details>
<summary><strong>Answers</strong></summary>

1. **Build Heap O(n):**
   $$\sum_{h=0}^{\log n} \lceil \frac{n}{2^{h+1}} \rceil \cdot O(h) = O(n) \cdot \sum_{h=0}^{\infty} \frac{h}{2^h} = O(n) \cdot 2 = O(n)$$

2. **Not stable example:**
   ```
   [5a, 5b, 3]
   Build max-heap: [5a, 5b, 3] or [5b, 5a, 3] (depends on comparisons)
   After sort: could end up [3, 5b, 5a] - order changed
   ```

3. **Choose Heap Sort when:**
   - Need O(1) space AND guaranteed O(n log n)
   - Real-time systems needing predictable timing
   - Partial sorting (Top K)

4. **Space complexity:**
   - Iterative: O(1)
   - Recursive: O(log n) for call stack

</details>

---

## üé§ Interview Context

<details>
<summary><strong>Common Interview Questions</strong></summary>

**Q: Why is build heap O(n)?**
"Most nodes are at the bottom levels where heapify cost is low. The mathematical analysis shows the work sums to O(n) due to the geometric series converging."

**Q: Why is Heap Sort not commonly used?**
"Despite guaranteed O(n log n), it has poor cache locality due to jumping between parent/child indices. Quick Sort with good pivot selection is typically 2-3x faster in practice."

**Q: When would you use Heap Sort?**
"When I need O(1) space and guaranteed worst-case O(n log n), like in embedded systems or real-time applications. Also for Top K problems where I only need k elements."

</details>

---

## ‚è±Ô∏è Time Estimates

| Activity | Time | Notes |
|----------|------|-------|
|" Understand build heap O(n) proof "| 20-30 min | Mathematical foundation |
| Compare with other sorts | 15-20 min | Trade-off analysis |
| Explain complexity in interview | 5-10 min | Practice articulation |
|" Answer "Why O(n)?" question "| 2-3 min | Common interview question |

---

## üí° Key Insight

> **Build Heap is O(n), not O(n log n).** This is because most nodes are near the leaves where heapify takes O(1). The total work forms a convergent series. Understanding this is crucial for analyzing algorithms that use heaps as building blocks.

---

## üîó Related

- **Previous:** [Heap Sort Algorithm](./8.1-Heap-Sort-Algorithm.md)
- **Foundation:** [Heapify](../02-Heap-Operations/2.4-Heapify.md)
- **Patterns:** [Top K](../04-Top-K-Pattern/4.1-Top-K-Overview.md) (uses partial heap sort concept)
