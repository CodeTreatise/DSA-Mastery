# Cache Fundamentals

> **Understanding caching principles before diving into implementations.**
>
> Caches are everywhere: CPU, databases, web browsers, CDNs. Understanding the fundamentals helps you design better systems.

---

## ðŸŽ¯ What is a Cache?

A **cache** is a temporary storage layer that stores frequently accessed data for faster retrieval.

```
Without Cache:          With Cache:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”
â”‚ Clientâ”‚              â”‚ Clientâ”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”˜              â””â”€â”€â”€â”¬â”€â”€â”€â”˜
    â”‚ slow                 â”‚ fast (hit)
    â–¼                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”
â”‚  DB   â”‚              â”‚ Cache â”‚â”€â”€â”
â””â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”¬â”€â”€â”€â”˜  â”‚ slow (miss)
                           â”‚      â”‚
                       â”Œâ”€â”€â”€â–¼â”€â”€â”€â”  â”‚
                       â”‚  DB   â”‚â—„â”€â”˜
                       â””â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ“ Key Concepts

### Cache Hit vs Miss

| Term | Definition | Performance |
|------|------------|-------------|
| **Cache Hit** | Data found in cache | O(1) |
| **Cache Miss** | Data not in cache, fetch from source | O(source) |
| **Hit Ratio** | Hits / (Hits + Misses) | Higher = better |

### Cache Operations

| Operation | Description | Typical Time |
|-----------|-------------|--------------|
| `get(key)` | Retrieve value if exists | O(1) |
| `put(key, value)` | Store or update value | O(1) |
| `evict()` | Remove item(s) to make space | Depends on policy |

---

## ðŸ”„ Eviction Policies

When cache is full, which item to remove?

| Policy | Evicts | Best For | Complexity |
|--------|--------|----------|------------|
| **LRU** (Least Recently Used) | Oldest access | General purpose | O(1) |
| **LFU** (Least Frequently Used) | Lowest access count | Frequency matters | O(1) or O(log n) |
| **FIFO** (First In, First Out) | Oldest insertion | Simple cases | O(1) |
| **Random** | Random item | When access is uniform | O(1) |
| **LRU-K** | Least recent K-th access | Database buffers | O(log n) |

### When to Use Each

```
LRU: "Recent accesses predict future accesses"
     Example: Browser history, file system cache

LFU: "Frequently accessed items are more valuable"
     Example: CDN caching popular videos

FIFO: "Age matters more than access pattern"
      Example: Logging buffers

Random: "No access pattern predictability"
        Example: Cryptographic applications
```

---

## ðŸ’¡ The Interview Problem

The classic interview cache design question:

> Design a data structure that supports `get(key)` and `put(key, value)` in O(1) time, with LRU eviction when capacity is exceeded.

### Why O(1) is Tricky

**Challenge 1:** Need O(1) lookup by key  
**Solution:** Use a HashMap

**Challenge 2:** Need O(1) removal of LRU item  
**Solution:** Track order somehow

**Challenge 3:** Need O(1) update of recency on access  
**Solution:** Can't use arrays (shifting is O(n))

**The Magic Combination:**
```
HashMap + Doubly Linked List = O(1) for everything!

HashMap: key â†’ Node pointer (O(1) access to any node)
DLL: Ordered by recency (O(1) move to front, O(1) remove from end)
```

---

## ðŸ“Š Cache Performance

### Hit Ratio Analysis

```python
# Simulating cache performance
def simulate_cache(requests, cache_size, policy="LRU"):
    hits = 0
    misses = 0
    cache = LRUCache(cache_size) if policy == "LRU" else ...
    
    for key in requests:
        if cache.get(key) != -1:
            hits += 1
        else:
            misses += 1
            cache.put(key, value)
    
    return hits / (hits + misses)

# Different workloads favor different policies:
# - Temporal locality â†’ LRU works well
# - Frequency skew â†’ LFU works well
# - Random access â†’ No policy helps much
```

### Real-World Hit Ratios

| System | Typical Hit Ratio | Cache Size |
|--------|-------------------|------------|
| CPU L1 Cache | 95-99% | 32-64 KB |
| CPU L2 Cache | 90-95% | 256 KB - 1 MB |
| Database Buffer | 90-99% | GB range |
| CDN Edge | 80-95% | TB range |
| Browser Cache | 60-80% | MB range |

---

## ðŸŽ¤ Interview Design Questions

### System Design Variations

1. **Distributed Cache**
   - How to handle cache consistency across nodes?
   - Partition strategy: hash-based vs range-based?

2. **TTL (Time-To-Live) Cache**
   - Items expire after fixed time
   - Need lazy or active expiration?

3. **Write Policies**
   - Write-through: Write to cache AND source
   - Write-back: Write to cache, sync later
   - Write-around: Write to source, cache on read

4. **Multi-Level Cache**
   - L1 (fast, small) + L2 (slower, larger)
   - When to promote/demote items?

---

## âš ï¸ Common Pitfalls

### 1. Cache Stampede

```
When cache expires, many requests hit the database simultaneously.

Timeline:
t=0:  Cache has value, 1000 req/sec â†’ all cache hits
t=1:  Cache expires
t=2:  1000 requests ALL hit database â†’ overload!

Solutions:
- Stagger expiration times
- Lock while fetching (only one request fetches)
- Probabilistic early refresh
```

### 2. Cache Warming

```
Cold cache after restart = poor performance initially.

Solutions:
- Pre-populate cache with popular items on startup
- Gradual traffic increase after restart
- Persistent cache (survives restarts)
```

### 3. Cache Invalidation

```
"There are only two hard things in CS:
 cache invalidation and naming things."

Challenge: How to know when cached data is stale?

Approaches:
- TTL: Expire after fixed time
- Event-based: Invalidate on data change
- Version tags: Check version before using
```

---

## ðŸ’» Simple Cache Implementation

**Basic in-memory cache with size limit:**

**Python:**
```python
from collections import OrderedDict

class SimpleCache:
    """
    Basic LRU cache using OrderedDict.
    
    Python 3.7+ dict maintains insertion order,
    but OrderedDict has move_to_end() which is useful for LRU.
    """
    
    def __init__(self, capacity: int):
        self.capacity = capacity
        self.cache = OrderedDict()
    
    def get(self, key):
        if key not in self.cache:
            return -1
        
        # Move to end (most recent)
        self.cache.move_to_end(key)
        return self.cache[key]
    
    def put(self, key, value):
        if key in self.cache:
            # Update and move to end
            self.cache.move_to_end(key)
        
        self.cache[key] = value
        
        if len(self.cache) > self.capacity:
            # Remove oldest (first item)
            self.cache.popitem(last=False)


# Example
cache = SimpleCache(2)
cache.put("a", 1)
cache.put("b", 2)
print(cache.get("a"))   # 1, moves "a" to end
cache.put("c", 3)       # Evicts "b" (oldest)
print(cache.get("b"))   # -1 (evicted)
print(cache.get("a"))   # 1 (still there)
```

**JavaScript:**
```javascript
class SimpleCache {
    constructor(capacity) {
        this.capacity = capacity;
        this.cache = new Map();
    }
    
    get(key) {
        if (!this.cache.has(key)) return -1;
        
        // Delete and re-insert to move to end
        const value = this.cache.get(key);
        this.cache.delete(key);
        this.cache.set(key, value);
        return value;
    }
    
    put(key, value) {
        if (this.cache.has(key)) {
            this.cache.delete(key);
        }
        
        this.cache.set(key, value);
        
        if (this.cache.size > this.capacity) {
            // Delete first (oldest) item
            const firstKey = this.cache.keys().next().value;
            this.cache.delete(firstKey);
        }
    }
}
```

> **Note:** This is simpler but slightly slower than HashMap + DLL approach. For interviews, you should implement the full HashMap + Doubly Linked List version to demonstrate understanding.

---

## ðŸ“ Key Takeaways

| Concept | Importance |
|---------|------------|
| LRU = HashMap + Doubly Linked List | Must know for interviews |
| O(1) for get and put | The key constraint |
| Eviction policy choice | System design knowledge |
| Cache stampede/warming | Senior-level awareness |

---

## â±ï¸ Time Estimates

| Topic | Time | Notes |
|-------|------|-------|
| Understand caching | 20 min | Concepts and policies |
| OrderedDict solution | 15 min | Quick implementation |
| HashMap + DLL | 30 min | Full interview answer |
| LFU extension | 30 min | Advanced variation |

---

> **ðŸ’¡ Key Insight:** The beauty of LRU cache is that two simple data structures (HashMap + Doubly Linked List), when combined cleverly, achieve O(1) for all operations. Neither can do it alone!

> **ðŸ”— Related:** [LRU Implementation â†’](./2.2-LRU-Implementation.md) | [LFU Cache](./2.3-LFU-Cache.md)

---

**Next:** [LRU Implementation â†’](./2.2-LRU-Implementation.md)
