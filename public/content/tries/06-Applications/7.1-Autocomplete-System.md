# Design Autocomplete System

> **Real-world Trie application: Build a search autocomplete system that suggests top-k completions based on frequency.**

---

## ðŸ“‹ Problem Statement

**LeetCode 642 - Hard (Similar concept)**

Design a search autocomplete system for a search engine. Users may input a sentence (at least one word and ending with '#'). For each character they type (except '#'), return the top 3 historical hot sentences with the same prefix as what is currently typed.

```
Input:
AutocompleteSystem(["i love you", "island", "iroman", "i love leetcode"], [5,3,2,2])
Input: 'i'  â†’ ["i love you", "island", "i love leetcode"]
Input: ' '  â†’ ["i love you", "i love leetcode"]
Input: 'a'  â†’ []
Input: '#'  â†’ (saves "i a" with count 1)
```

**Requirements:**
- Return top 3 by frequency
- Tie-breaker: lexicographic order
- `#` ends input and saves the sentence

---

## ðŸŽ¯ Pattern Recognition

**Patterns Combined:**
- **Trie** for prefix matching
- **Heap/Sorting** for top-k selection
- **DFS** to collect all sentences with prefix

**Key insight:** Store frequency at end nodes, collect candidates, sort by frequency.

---

## ðŸ’» Implementation

### Solution: Trie + Frequency Tracking

**Python:**
```python
from collections import defaultdict
import heapq


class TrieNode:
    def __init__(self):
        self.children = {}
        self.sentences = defaultdict(int)  # sentence -> frequency


class AutocompleteSystem:
    """
    Autocomplete with frequency-based ranking.
    
    insert: O(m) where m is sentence length
    input: O(n Ã— log(n)) for sorting n matching sentences
    """
    
    def __init__(self, sentences: list[str], times: list[int]):
        self.root = TrieNode()
        self.current_input = ""  # Track what user is typing
        self.current_node = self.root  # Current position in Trie
        
        # Insert all historical sentences
        for sentence, count in zip(sentences, times):
            self._insert(sentence, count)
    
    def _insert(self, sentence: str, count: int) -> None:
        """Insert sentence with frequency into Trie."""
        node = self.root
        for char in sentence:
            if char not in node.children:
                node.children[char] = TrieNode()
            node = node.children[char]
            node.sentences[sentence] += count  # Track at every prefix
        # Also mark at end (optional, since we track at all nodes)
    
    def input(self, c: str) -> list[str]:
        """
        Process single character input.
        Returns top 3 suggestions or [] if no match.
        """
        if c == '#':
            # End of input, save the sentence
            self._insert(self.current_input, 1)
            self.current_input = ""
            self.current_node = self.root
            return []
        
        # Append character to current input
        self.current_input += c
        
        # Navigate Trie
        if self.current_node is None:
            return []  # Already dead-end
        
        if c not in self.current_node.children:
            self.current_node = None
            return []
        
        self.current_node = self.current_node.children[c]
        
        # Get top 3 sentences with this prefix
        return self._get_top_3()
    
    def _get_top_3(self) -> list[str]:
        """Get top 3 sentences from current node."""
        if self.current_node is None:
            return []
        
        # Get all sentences and their frequencies
        candidates = list(self.current_node.sentences.items())
        
        # Sort by (-frequency, sentence) for top-k
        candidates.sort(key=lambda x: (-x[1], x[0]))
        
        return [sentence for sentence, _ in candidates[:3]]


# Usage
auto = AutocompleteSystem(
    ["i love you", "island", "iroman", "i love leetcode"],
    [5, 3, 2, 2]
)
print(auto.input('i'))   # ["i love you", "island", "i love leetcode"]
print(auto.input(' '))   # ["i love you", "i love leetcode"]
print(auto.input('a'))   # []
print(auto.input('#'))   # [] (saves "i a")
```

**JavaScript:**
```javascript
class TrieNode {
    constructor() {
        this.children = new Map();
        this.sentences = new Map(); // sentence -> frequency
    }
}

class AutocompleteSystem {
    constructor(sentences, times) {
        this.root = new TrieNode();
        this.currentInput = "";
        this.currentNode = this.root;
        
        for (let i = 0; i < sentences.length; i++) {
            this._insert(sentences[i], times[i]);
        }
    }
    
    _insert(sentence, count) {
        let node = this.root;
        for (const char of sentence) {
            if (!node.children.has(char)) {
                node.children.set(char, new TrieNode());
            }
            node = node.children.get(char);
            node.sentences.set(
                sentence, 
                (node.sentences.get(sentence) || 0) + count
            );
        }
    }
    
    input(c) {
        if (c === '#') {
            this._insert(this.currentInput, 1);
            this.currentInput = "";
            this.currentNode = this.root;
            return [];
        }
        
        this.currentInput += c;
        
        if (this.currentNode === null) return [];
        
        if (!this.currentNode.children.has(c)) {
            this.currentNode = null;
            return [];
        }
        
        this.currentNode = this.currentNode.children.get(c);
        return this._getTop3();
    }
    
    _getTop3() {
        if (this.currentNode === null) return [];
        
        const candidates = [...this.currentNode.sentences.entries()];
        candidates.sort((a, b) => {
            if (b[1] !== a[1]) return b[1] - a[1]; // Higher freq first
            return a[0].localeCompare(b[0]); // Lexicographic for ties
        });
        
        return candidates.slice(0, 3).map(([s, _]) => s);
    }
}
```

---

### Optimized Solution: Heap for Large Data

**Python:**
```python
import heapq


class AutocompleteSystemOptimized:
    """
    Use min-heap for top-k selection when many sentences.
    """
    
    def _get_top_3_heap(self) -> list[str]:
        """Use heap for O(n log 3) instead of O(n log n) sort."""
        if self.current_node is None:
            return []
        
        # Min-heap of size 3
        # Heap item: (frequency, sentence) but inverted for min-heap behavior
        heap = []
        
        for sentence, freq in self.current_node.sentences.items():
            # For min-heap: push (-freq, sentence) to get max-freq at top
            # But we want min-heap of size 3, so push (freq, -lex_order, sentence)
            # Actually simpler: just use negative freq
            
            item = (-freq, sentence)  # -freq so higher freq is "smaller"
            
            if len(heap) < 3:
                heapq.heappush(heap, (-freq, sentence))  # -(-freq) = freq for comparison
            else:
                # Push then pop to maintain top 3
                # This is tricky with tie-breaking...
                pass  # For simplicity, sorting is fine for interview
        
        # Actually for interviews, the sorting approach is clearer
        candidates = list(self.current_node.sentences.items())
        candidates.sort(key=lambda x: (-x[1], x[0]))
        return [s for s, _ in candidates[:3]]
```

---

## âš¡ Complexity Analysis

| Operation | Time | Space |
|-----------|------|-------|
| Constructor | O(total_chars) | O(total_chars Ã— n) |
| input (navigate) | O(1) | O(1) |
| input (get top 3) | O(n log n) | O(n) |
| Save sentence | O(m) | O(m Ã— n) |

**Where:**
- n = number of matching sentences
- m = current sentence length

**Space note:** We store sentence at every prefix node, which is O(m) copies per sentence. Can optimize with just storing at end nodes + DFS.

---

## ðŸ”„ Trade-offs

### Store at Every Node vs End Only

| Approach | Pros | Cons |
|----------|------|------|
| Every node | O(1) retrieval | O(nÃ—m) space |
| End only + DFS | O(n) space | O(subtree) retrieval |

### Sort vs Heap

| Approach | Time | When |
|----------|------|------|
| Sort all | O(n log n) | Small n (<1000) |
| Heap top-k | O(n log k) | Large n, small k |

---

## âš ï¸ Edge Cases

| Case | Handling |
|------|----------|
| Empty input | Return [] |
| No matches | Return [] |
| Fewer than 3 matches | Return all matches |
| Space in input | Treat as regular character |
| Duplicate sentences | Accumulate frequency |

---

## ðŸŽ¤ Interview Tips

**Opening statement:**
"I'll use a Trie for prefix matching. At each node, I'll store all sentences with that prefix along with their frequencies. When the user types, I traverse the Trie and return the top 3 by frequency."

**Design decisions to discuss:**
1. "I store sentences at every prefix node for O(1) lookup, trading space for time."
2. "For top-k, sorting is O(n log n). Could use heap for O(n log k) with large data."
3. "The '#' character signals end of input and saves the new sentence."

**Follow-up questions:**
- "What if we need real-time updates?" â†’ Already handled
- "Scale to millions of sentences?" â†’ Consider distributed Trie
- "Personalization?" â†’ Per-user frequency tables

---

## ðŸ“ Practice Checklist

- [ ] Implement basic autocomplete with Trie
- [ ] Handle tie-breaking correctly
- [ ] Track current position during input
- [ ] Reset state on '#'
- [ ] Understand space/time trade-offs

<details>
<summary><strong>ðŸ§  Spaced Repetition</strong></summary>

- **Day 1:** Implement from scratch
- **Day 3:** Explain the design choices
- **Day 7:** Add optimization (heap)
- **Day 14:** Design similar system (tag suggestions)

</details>

---

> **ðŸ’¡ Key Insight:** Storing sentences at every prefix node trades space for O(1) retrieval time. For large-scale systems, use DFS from prefix node instead.

> **ðŸ”— Related:** [Collect All Words](../03-Trie-Patterns/4.3-Collect-All-Words.md) | [Search Suggestions](../07-Practice-Problems/6.4-Search-Suggestions.md)
