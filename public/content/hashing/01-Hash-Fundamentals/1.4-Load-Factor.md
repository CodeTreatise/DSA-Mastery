# Load Factor and Resizing

> **Load factor measures hash table fullness and triggers resizing to maintain O(1) performance.**

The load factor is the ratio of stored elements to bucket count. When it gets too high, collisions increase dramatically, degrading performance from O(1) to O(n). Understanding load factor is essential for implementing efficient hash tables.

---

## üéØ Pattern Recognition

<details>
<summary><strong>Understanding Load Factor</strong></summary>

**Definition:**
```
Load Factor (Œ±) = n / m

Where:
  n = number of stored elements
  m = number of buckets (table size)
```

**Why it matters:**
- Œ± = 0.5 ‚Üí 50% full ‚Üí few collisions
- Œ± = 0.7 ‚Üí 70% full ‚Üí acceptable
- Œ± = 1.0 ‚Üí 100% full ‚Üí many collisions
- Œ± > 1.0 ‚Üí only possible with chaining

**Industry standards:**
- Java HashMap: resize at Œ± = 0.75
- Python dict: resize at Œ± = 0.67
- C++ unordered_map: resize at Œ± = 1.0

</details>

---

## ‚úÖ Choosing the Right Load Factor

| Load Factor | Pros | Cons | Use When |
|-------------|------|------|----------|
| Œ± < 0.5 | Very fast | Memory wasteful | Speed critical |
| Œ± = 0.5-0.7 | Balanced | Good trade-off | General use |
| Œ± = 0.75 | Standard | Default choice | Most cases |
| Œ± > 0.9 | Memory efficient | Slower | Memory constrained |

## ‚ùå When Load Factor Matters Most

- **Open addressing:** Must resize before Œ± = 1.0 (table full)
- **Chaining:** Can exceed 1.0, but performance degrades
- **Real-time systems:** Keep Œ± low for predictable performance
- **Memory-constrained:** Accept higher Œ± for space savings

---

## üîó Concept Map

<details>
<summary><strong>Prerequisites & Next Steps</strong></summary>

**Before this, know:**
- [What Is Hashing](./1.1-What-Is-Hashing.md)
- [Collision Handling](./1.3-Collision-Handling.md)
- Amortized analysis basics

**After this, learn:**
- [Design HashMap (LC 706)](../08-Design-Problems/8.2-Design-HashMap-LC706.md)
- Dynamic array resizing (similar concept)
- Consistent hashing for distributed systems

**Related concepts:**
- Amortized O(1) insertion
- Rehashing complexity
- Prime number bucket sizing

</details>

---

## üìê How It Works

### Load Factor Impact on Performance

```
Expected probe length with linear probing:

Œ± = 0.5: ~1.5 probes on average
Œ± = 0.7: ~2.2 probes
Œ± = 0.9: ~5.5 probes
Œ± = 0.99: ~50 probes!

Formula: E[probes] ‚âà 1/(1-Œ±) for unsuccessful search
```

### Visualization: Collisions vs Load Factor

```
Load Factor ‚Üí Collision Probability

Œ± = 0.25  ‚ñì‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  Few collisions
Œ± = 0.50  ‚ñì‚ñì‚ñì‚ñì‚ñì‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  Some collisions
Œ± = 0.75  ‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  More collisions
Œ± = 0.90  ‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñë‚ñë‚ñë  Many collisions
Œ± = 0.99  ‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì  Severe clustering
```

### When to Resize

```
Hash Table Lifecycle:

Initial: m = 16, n = 0, Œ± = 0

After insertions:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ n    ‚îÇ m    ‚îÇ Œ±      ‚îÇ Action          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 1    ‚îÇ 16   ‚îÇ 0.06   ‚îÇ Normal insert   ‚îÇ
‚îÇ 8    ‚îÇ 16   ‚îÇ 0.50   ‚îÇ Normal insert   ‚îÇ
‚îÇ 12   ‚îÇ 16   ‚îÇ 0.75   ‚îÇ Threshold!      ‚îÇ
‚îÇ 12   ‚îÇ 32   ‚îÇ 0.375  ‚îÇ After resize    ‚îÇ
‚îÇ 24   ‚îÇ 32   ‚îÇ 0.75   ‚îÇ Threshold again ‚îÇ
‚îÇ 24   ‚îÇ 64   ‚îÇ 0.375  ‚îÇ After resize    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Resizing Process

```
Before Resize (m=4, n=3, Œ±=0.75):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ [0]: "dog"                    ‚îÇ
‚îÇ [1]: "cat" ‚Üí "bat"  (chain)   ‚îÇ
‚îÇ [2]: empty                    ‚îÇ
‚îÇ [3]: "ant"                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

After Resize (m=8, n=3, Œ±=0.375):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ [0]: "dog"                    ‚îÇ  ‚Üê Rehashed!
‚îÇ [1]: empty                    ‚îÇ
‚îÇ [2]: empty                    ‚îÇ
‚îÇ [3]: "ant"                    ‚îÇ  ‚Üê Same or different
‚îÇ [4]: empty                    ‚îÇ
‚îÇ [5]: "cat"                    ‚îÇ  ‚Üê No longer colliding!
‚îÇ [6]: empty                    ‚îÇ
‚îÇ [7]: "bat"                    ‚îÇ  ‚Üê Split from chain
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

All elements must be rehashed because:
  hash(key) % 4 ‚â† hash(key) % 8
```

---

## üíª Code Implementation

### Python - Hash Table with Resizing

```python
class ResizableHashTable:
    """
    Hash table with automatic resizing based on load factor.
    Uses chaining for collision resolution.
    """
    
    DEFAULT_CAPACITY = 16
    LOAD_FACTOR_THRESHOLD = 0.75
    SHRINK_THRESHOLD = 0.25
    
    def __init__(self, capacity: int = DEFAULT_CAPACITY):
        self.capacity = capacity
        self.size = 0
        self.buckets = [[] for _ in range(capacity)]
    
    def _hash(self, key) -> int:
        return hash(key) % self.capacity
    
    def _load_factor(self) -> float:
        return self.size / self.capacity
    
    def _resize(self, new_capacity: int) -> None:
        """
        Resize and rehash all elements.
        
        Time: O(n) - must rehash every element
        Amortized: O(1) per insertion over time
        """
        old_buckets = self.buckets
        
        self.capacity = new_capacity
        self.size = 0
        self.buckets = [[] for _ in range(new_capacity)]
        
        # Rehash all existing elements
        for bucket in old_buckets:
            for key, value in bucket:
                self.put(key, value)
    
    def _check_resize(self) -> None:
        """Check if resize is needed and perform it."""
        if self._load_factor() > self.LOAD_FACTOR_THRESHOLD:
            # Double the capacity
            self._resize(self.capacity * 2)
        elif (self._load_factor() < self.SHRINK_THRESHOLD and 
              self.capacity > self.DEFAULT_CAPACITY):
            # Halve the capacity (don't go below default)
            self._resize(self.capacity // 2)
    
    def put(self, key, value) -> None:
        """Insert or update key-value pair."""
        self._check_resize()
        
        index = self._hash(key)
        bucket = self.buckets[index]
        
        # Check for existing key
        for i, (k, v) in enumerate(bucket):
            if k == key:
                bucket[i] = (key, value)
                return
        
        # New key
        bucket.append((key, value))
        self.size += 1
    
    def get(self, key, default=None):
        """Get value by key."""
        index = self._hash(key)
        bucket = self.buckets[index]
        
        for k, v in bucket:
            if k == key:
                return v
        
        return default
    
    def remove(self, key) -> bool:
        """Remove key-value pair."""
        index = self._hash(key)
        bucket = self.buckets[index]
        
        for i, (k, v) in enumerate(bucket):
            if k == key:
                del bucket[i]
                self.size -= 1
                self._check_resize()  # Maybe shrink
                return True
        
        return False
    
    def stats(self) -> dict:
        """Return statistics about the hash table."""
        chain_lengths = [len(bucket) for bucket in self.buckets]
        return {
            'size': self.size,
            'capacity': self.capacity,
            'load_factor': self._load_factor(),
            'max_chain': max(chain_lengths),
            'empty_buckets': chain_lengths.count(0),
        }


# Usage demonstration
ht = ResizableHashTable(4)  # Start small

# Insert elements and observe resizing
for i in range(10):
    ht.put(f"key{i}", i)
    print(f"After insert {i}: {ht.stats()}")
```

### Python - Open Addressing with Resizing

```python
class OpenAddressingHashTable:
    """Hash table with linear probing and resizing."""
    
    LOAD_FACTOR_THRESHOLD = 0.7  # Must be < 1 for open addressing
    DELETED = object()
    
    def __init__(self, capacity: int = 16):
        self.capacity = capacity
        self.size = 0
        self.keys = [None] * capacity
        self.values = [None] * capacity
    
    def _hash(self, key) -> int:
        return hash(key) % self.capacity
    
    def _resize(self, new_capacity: int) -> None:
        """Resize with full rehashing."""
        old_keys = self.keys
        old_values = self.values
        
        self.capacity = new_capacity
        self.size = 0
        self.keys = [None] * new_capacity
        self.values = [None] * new_capacity
        
        for i, key in enumerate(old_keys):
            if key is not None and key is not self.DELETED:
                self.put(key, old_values[i])
    
    def _probe(self, key, for_insert: bool = False) -> int:
        """Linear probing to find slot."""
        index = self._hash(key)
        start = index
        first_deleted = None
        
        while True:
            if self.keys[index] is None:
                return first_deleted if (for_insert and first_deleted) else index
            
            if self.keys[index] is self.DELETED:
                if first_deleted is None:
                    first_deleted = index
            elif self.keys[index] == key:
                return index
            
            index = (index + 1) % self.capacity
            if index == start:
                return first_deleted if first_deleted else -1
    
    def put(self, key, value) -> None:
        """Insert with resize check."""
        # Must resize BEFORE table is full
        if self.size >= self.capacity * self.LOAD_FACTOR_THRESHOLD:
            self._resize(self.capacity * 2)
        
        index = self._probe(key, for_insert=True)
        
        if self.keys[index] is None or self.keys[index] is self.DELETED:
            self.size += 1
        
        self.keys[index] = key
        self.values[index] = value
    
    def get(self, key, default=None):
        index = self._probe(key)
        if index >= 0 and self.keys[index] == key:
            return self.values[index]
        return default


# Demonstrate load factor behavior
ht = OpenAddressingHashTable(10)
for i in range(10):
    ht.put(f"key{i}", i)
    load = ht.size / ht.capacity
    print(f"n={ht.size}, m={ht.capacity}, Œ±={load:.2f}")
```

### JavaScript - Resizable Hash Table

```javascript
class ResizableHashTable {
    static DEFAULT_CAPACITY = 16;
    static LOAD_FACTOR_THRESHOLD = 0.75;
    
    constructor(capacity = ResizableHashTable.DEFAULT_CAPACITY) {
        this.capacity = capacity;
        this.size = 0;
        this.buckets = Array.from({ length: capacity }, () => []);
    }
    
    _hash(key) {
        let hash = 0;
        for (const char of String(key)) {
            hash = (hash * 31 + char.charCodeAt(0)) % this.capacity;
        }
        return hash;
    }
    
    _loadFactor() {
        return this.size / this.capacity;
    }
    
    _resize(newCapacity) {
        const oldBuckets = this.buckets;
        
        this.capacity = newCapacity;
        this.size = 0;
        this.buckets = Array.from({ length: newCapacity }, () => []);
        
        // Rehash all elements
        for (const bucket of oldBuckets) {
            for (const [key, value] of bucket) {
                this.put(key, value);
            }
        }
    }
    
    put(key, value) {
        // Check for resize
        if (this._loadFactor() > ResizableHashTable.LOAD_FACTOR_THRESHOLD) {
            this._resize(this.capacity * 2);
        }
        
        const index = this._hash(key);
        const bucket = this.buckets[index];
        
        // Check for existing key
        for (let i = 0; i < bucket.length; i++) {
            if (bucket[i][0] === key) {
                bucket[i][1] = value;
                return;
            }
        }
        
        bucket.push([key, value]);
        this.size++;
    }
    
    get(key) {
        const index = this._hash(key);
        const bucket = this.buckets[index];
        
        for (const [k, v] of bucket) {
            if (k === key) return v;
        }
        
        return undefined;
    }
    
    stats() {
        const chainLengths = this.buckets.map(b => b.length);
        return {
            size: this.size,
            capacity: this.capacity,
            loadFactor: this._loadFactor().toFixed(2),
            maxChain: Math.max(...chainLengths),
            emptyBuckets: chainLengths.filter(l => l === 0).length
        };
    }
}

// Usage
const ht = new ResizableHashTable(4);
for (let i = 0; i < 10; i++) {
    ht.put(`key${i}`, i);
    console.log(`After insert ${i}:`, ht.stats());
}
```

---

## ‚ö° Complexity Analysis

### Amortized Analysis of Resizing

| Operation | Time (amortized) | Worst Case | Notes |
|-----------|------------------|------------|-------|
| Insert | O(1) | O(n) | Resize happens |
| Lookup | O(1) | O(n) | Rare with good Œ± |
| Delete | O(1) | O(n) | With shrinking |
| Resize | - | O(n) | Copies all elements |

**Why O(1) amortized insertion:**
```
Insert costs: 1, 1, 1, resize(4), 1, 1, 1, 1, resize(8), ...

Total cost for n insertions:
= n (regular inserts) + 4 + 8 + 16 + ... + n
= n + 2n = 3n

Amortized cost = 3n / n = O(1)
```

### Space Complexity

| Load Factor | Space Efficiency | Notes |
|-------------|------------------|-------|
| Œ± = 0.25 | 25% utilized | Wasteful |
| Œ± = 0.50 | 50% utilized | Acceptable |
| Œ± = 0.75 | 75% utilized | Good trade-off |

**Memory usage:**
- Chaining: O(n) for elements + O(m) for buckets + pointer overhead
- Probing: O(m) for array (m ‚â• n/Œ±)

---

## üîÑ Resizing Strategies

| Strategy | When to Apply | Pros | Cons |
|----------|---------------|------|------|
| Double size | Œ± > threshold | Simple, effective | Memory spikes |
| Incremental | Each insert | Smooth | Slower inserts |
| Prime sizes | Always | Better distribution | Complex sizing |
| Power of 2 | Common | Fast modulo (bit AND) | May affect distribution |

---

## ‚ö†Ô∏è Common Mistakes

### 1. Not Resizing Before Full (Open Addressing)

```python
# ‚ùå WRONG: Resize only when completely full
if self.size == self.capacity:
    self._resize()  # Too late! Performance already degraded

# ‚úÖ CORRECT: Resize before too full
if self.size >= self.capacity * 0.7:
    self._resize()
```

### 2. Using Wrong Capacity After Resize

```python
# ‚ùå WRONG: Hash with old capacity
def put(self, key, value):
    self._resize_if_needed()
    index = hash(key) % self.old_capacity  # Bug!

# ‚úÖ CORRECT: Always use current capacity
def put(self, key, value):
    self._resize_if_needed()
    index = hash(key) % self.capacity  # Current capacity
```

### 3. Forgetting to Rehash

```python
# ‚ùå WRONG: Just copying elements
def _resize(self, new_capacity):
    self.buckets.extend([[] for _ in range(new_capacity - self.capacity)])
    self.capacity = new_capacity
    # Elements now in wrong buckets!

# ‚úÖ CORRECT: Rehash everything
def _resize(self, new_capacity):
    old_buckets = self.buckets
    self.buckets = [[] for _ in range(new_capacity)]
    self.capacity = new_capacity
    self.size = 0
    for bucket in old_buckets:
        for key, value in bucket:
            self.put(key, value)  # Rehash with new capacity
```

### 4. Resizing During Iteration

```python
# ‚ùå WRONG: Modifying while iterating
for key in hash_table:
    hash_table.put(new_key, value)  # May trigger resize!

# ‚úÖ CORRECT: Collect modifications first
to_add = []
for key in hash_table:
    to_add.append((new_key, value))
for key, value in to_add:
    hash_table.put(key, value)
```

---

## üìù Practice Problems (Progressive)

| Problem | Difficulty | Focus | Link |
|---------|------------|-------|------|
| Design HashMap | üü¢ Easy | Basic resizing | [LC 706](https://leetcode.com/problems/design-hashmap/) |
| Design HashSet | üü¢ Easy | Set with resize | [LC 705](https://leetcode.com/problems/design-hashset/) |
| LRU Cache | üü° Medium | Hash + resizing | [LC 146](https://leetcode.com/problems/lru-cache/) |
| Randomized Set | üü° Medium | O(1) operations | [LC 380](https://leetcode.com/problems/insert-delete-getrandom-o1/) |

<details>
<summary><strong>üß† Spaced Repetition Schedule</strong></summary>

- **Day 1:** Understand load factor math
- **Day 3:** Implement hash table with resizing
- **Day 7:** Analyze amortized complexity
- **Day 14:** Implement with shrinking on delete
- **Day 30:** Explain trade-offs clearly

</details>

---

## üé§ Interview Context

<details>
<summary><strong>Load Factor in Interviews</strong></summary>

**When this comes up:**
- "Implement a hash map from scratch"
- "What's the time complexity of insertion?"
- "Why is it amortized O(1)?"

**Key points:**
> "I'll use a load factor of 0.75. When we exceed that, I double the capacity and rehash all elements."

**Explaining amortized O(1):**
> "Resize happens rarely - we double the size each time, so the total cost of resizing over n insertions is O(n), giving O(1) per insertion amortized."

**Trade-off discussion:**
> "Lower load factor means faster operations but more memory. Higher load factor saves memory but increases collisions."

</details>

**Company Focus:**

| Company | Load Factor Knowledge | Notes |
|---------|----------------------|-------|
| Google | Expect understanding | System design |
| Amazon | May ask in depth | Scaling context |
| Meta | Basic understanding | Usually OK |
| Bloomberg | Implementation | More common |

---

## ‚è±Ô∏è Time Estimates

| Activity | Time |
|----------|------|
| Understand load factor concept | 15 min |
| Implement resizing | 25 min |
| Understand amortized analysis | 20 min |
| Add shrinking on delete | 15 min |

---

> **üí° Key Insight:** Load factor balances space and time. The magic of amortized O(1) comes from doubling capacity - we pay O(n) for resize but get n free insertions, averaging to O(1) per insert.

> **üîó Related:** [Collision Handling](./1.3-Collision-Handling.md) | [Design HashMap](../08-Design-Problems/8.2-Design-HashMap-LC706.md) | [Amortized Analysis](../../00-Prerequisites.md#complexity)
