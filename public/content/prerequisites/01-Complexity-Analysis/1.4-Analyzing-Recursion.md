# 1.4 Analyzing Recursion

> **How to find the complexity of recursive algorithms using trees and formulas.**
>
> ‚è±Ô∏è *Estimated reading: 30 minutes*

---

## üéØ Why Recursion Complexity Is Different

Loops are straightforward: count iterations √ó work per iteration.

Recursion is trickier because:
1. The function calls **itself** multiple times
2. Each call may spawn **more calls** (branching)
3. Work is done at **different levels**

We need new tools: **Recursion Trees** and the **Master Theorem**.

---

## üìê The Recursion Tree Method

### The Core Idea

Draw every recursive call as a tree:
- **Root:** The initial call
- **Children:** The recursive calls made
- **Work:** Operations done at each node

Then sum up all the work across the entire tree.

### Example 1: Simple Recursion ‚Äî O(n)

```python
def countdown(n):
    if n <= 0:           # Base case: O(1)
        return
    print(n)             # Work: O(1)
    countdown(n - 1)     # One recursive call
```

**Tree:**
```
countdown(5)     ‚Üê 1 unit of work
    ‚îÇ
countdown(4)     ‚Üê 1 unit of work
    ‚îÇ
countdown(3)     ‚Üê 1 unit of work
    ‚îÇ
countdown(2)     ‚Üê 1 unit of work
    ‚îÇ
countdown(1)     ‚Üê 1 unit of work
    ‚îÇ
countdown(0)     ‚Üê base case
```

**Analysis:**
- Tree height: n
- Work per level: O(1)
- Total: n √ó O(1) = **O(n)**

### Example 2: Binary Recursion ‚Äî O(2‚Åø)

```python
def fibonacci(n):
    if n <= 1:
        return n
    return fibonacci(n - 1) + fibonacci(n - 2)
```

**Tree for fib(5):**
```
                    fib(5)                         Level 0: 1 call
                   /      \
              fib(4)      fib(3)                   Level 1: 2 calls
             /    \        /    \
        fib(3)  fib(2)  fib(2)  fib(1)             Level 2: 4 calls
        /   \    /   \    /   \
    fib(2) fib(1)fib(1)fib(0)fib(1)fib(0)          Level 3: ~8 calls
    /   \
fib(1) fib(0)                                      Level 4: continues...
```

**Analysis:**
- At each level, calls roughly **double**
- Tree height: ~n (actually Œò(n))
- Calls at level k: ~2^k
- Total calls: 1 + 2 + 4 + ... + 2^n ‚âà **O(2‚Åø)**

### Example 3: Divide and Conquer ‚Äî O(n log n)

```python
def merge_sort(arr):
    if len(arr) <= 1:
        return arr
    
    mid = len(arr) // 2
    left = merge_sort(arr[:mid])    # T(n/2)
    right = merge_sort(arr[mid:])   # T(n/2)
    
    return merge(left, right)        # O(n) merge work
```

**Tree:**
```
                merge_sort(n)              O(n) work (merge)
               /           \
      merge_sort(n/2)  merge_sort(n/2)     O(n) work total
         /    \           /    \
    T(n/4) T(n/4)    T(n/4) T(n/4)         O(n) work total
      ...                                   ...

   (log n levels total)
```

**Analysis:**
- Tree height: log‚ÇÇ(n) (we halve each time)
- Work per level: O(n) (all merges at each level sum to n)
- Total: log‚ÇÇ(n) √ó O(n) = **O(n log n)**

### Example 4: Binary Search ‚Äî O(log n)

```python
def binary_search(arr, target, left, right):
    if left > right:
        return -1
    
    mid = (left + right) // 2
    
    if arr[mid] == target:
        return mid
    elif arr[mid] < target:
        return binary_search(arr, target, mid + 1, right)  # Only ONE call
    else:
        return binary_search(arr, target, left, mid - 1)   # Only ONE call
```

**Tree:**
```
binary_search(n)     ‚Üê O(1) work
        ‚îÇ
binary_search(n/2)   ‚Üê O(1) work
        ‚îÇ
binary_search(n/4)   ‚Üê O(1) work
        ‚îÇ
       ...
        ‚îÇ
binary_search(1)     ‚Üê O(1) work
```

**Analysis:**
- Tree height: log‚ÇÇ(n)
- Only ONE branch (linear path)
- Work per node: O(1)
- Total: log‚ÇÇ(n) √ó O(1) = **O(log n)**

---

## üìä The Recurrence Relation

A **recurrence relation** describes the time complexity mathematically:

### Common Recurrence Patterns

| Recurrence | Solution | Example |
|------------|----------|---------|
| T(n) = T(n-1) + O(1) | O(n) | Factorial |
| T(n) = T(n-1) + O(n) | O(n¬≤) | Selection sort |
| T(n) = 2T(n-1) + O(1) | O(2‚Åø) | Fibonacci (naive) |
| T(n) = T(n/2) + O(1) | O(log n) | Binary search |
| T(n) = T(n/2) + O(n) | O(n) | Some divide & conquer |
| T(n) = 2T(n/2) + O(1) | O(n) | Tree traversal |
| T(n) = 2T(n/2) + O(n) | O(n log n) | Merge sort |
| T(n) = 2T(n/2) + O(n¬≤) | O(n¬≤) | Some matrix algorithms |

### How to Write a Recurrence

1. **Identify the number of recursive calls** ‚Üí multiplier
2. **Identify the subproblem size** ‚Üí what n becomes
3. **Identify the work done outside recursion** ‚Üí the "+ O(f(n))" part

**Example: Merge Sort**
```python
def merge_sort(arr):       # Total time: T(n)
    ...
    left = merge_sort(arr[:mid])    # T(n/2)
    right = merge_sort(arr[mid:])   # T(n/2)
    return merge(left, right)        # O(n)
```

Recurrence: **T(n) = 2T(n/2) + O(n)**

---

## üßÆ The Master Theorem (Simplified)

The Master Theorem provides a **shortcut** for divide-and-conquer recurrences of the form:

```
T(n) = aT(n/b) + O(n^d)
```

Where:
- **a** = number of subproblems (recursive calls)
- **b** = factor by which input shrinks
- **d** = exponent in the work done outside recursion

### The Three Cases

| Compare a to b^d | Result | Example |
|------------------|--------|---------|
| **a < b^d** | O(n^d) | Work at root dominates |
| **a = b^d** | O(n^d log n) | Work spread evenly |
| **a > b^d** | O(n^(log_b(a))) | Leaves dominate |

### Case 1: Root Dominates (a < b^d)

```python
def case1(n):
    if n <= 1:
        return
    case1(n // 2)      # a = 1, b = 2
    # O(n) work         # d = 1
    for i in range(n):
        print(i)

# a = 1, b = 2, d = 1
# Compare: 1 vs 2¬π = 2
# 1 < 2, so Case 1: O(n^d) = O(n)
```

**Intuition:** The work at the top level (O(n)) is more than all the recursive work combined.

### Case 2: Equal Split (a = b^d)

```python
def merge_sort(arr):  # T(n) = 2T(n/2) + O(n)
    ...
    left = merge_sort(arr[:mid])    # a = 2, b = 2
    right = merge_sort(arr[mid:])   
    return merge(left, right)        # d = 1, O(n)

# a = 2, b = 2, d = 1
# Compare: 2 vs 2¬π = 2
# 2 = 2, so Case 2: O(n^d log n) = O(n log n)
```

**Intuition:** Work is evenly distributed across log n levels.

### Case 3: Leaves Dominate (a > b^d)

```python
def case3(n):
    if n <= 1:
        return
    case3(n // 2)      # a = 4, b = 2
    case3(n // 2)
    case3(n // 2)
    case3(n // 2)
    print("work")       # d = 0, O(1)

# a = 4, b = 2, d = 0
# Compare: 4 vs 2‚Å∞ = 1
# 4 > 1, so Case 3: O(n^(log‚ÇÇ4)) = O(n¬≤)
```

**Intuition:** The number of leaves grows faster than the work at each level shrinks.

---

## üìã Quick Reference: Common Recursive Algorithms

| Algorithm | Recurrence | Complexity |
|-----------|------------|------------|
| **Binary Search** | T(n) = T(n/2) + O(1) | O(log n) |
| **Merge Sort** | T(n) = 2T(n/2) + O(n) | O(n log n) |
| **Quick Sort (avg)** | T(n) = 2T(n/2) + O(n) | O(n log n) |
| **Quick Sort (worst)** | T(n) = T(n-1) + O(n) | O(n¬≤) |
| **Tree Traversal** | T(n) = 2T(n/2) + O(1) | O(n) |
| **Fibonacci (naive)** | T(n) = T(n-1) + T(n-2) + O(1) | O(2‚Åø) |
| **Fibonacci (memoized)** | T(n) = O(1) per state | O(n) |
| **Factorial** | T(n) = T(n-1) + O(1) | O(n) |
| **Power (naive)** | T(n) = T(n-1) + O(1) | O(n) |
| **Power (fast)** | T(n) = T(n/2) + O(1) | O(log n) |
| **Subsets** | T(n) = 2T(n-1) + O(1) | O(2‚Åø) |
| **Permutations** | T(n) = n √ó T(n-1) | O(n!) |

---

## üíª Detailed Examples

### Example 1: Factorial

```python
def factorial(n):
    if n <= 1:           # O(1)
        return 1
    return n * factorial(n - 1)  # T(n-1) + O(1)
```

**Recurrence:** T(n) = T(n-1) + O(1)

**Tree:**
```
factorial(n) ‚Üí factorial(n-1) ‚Üí factorial(n-2) ‚Üí ... ‚Üí factorial(1)
     O(1)           O(1)             O(1)                  O(1)
```

- n levels, O(1) each
- Total: **O(n)**

### Example 2: Exponentiation

**Naive approach:**
```python
def power_naive(x, n):
    if n == 0:
        return 1
    return x * power_naive(x, n - 1)  # T(n-1) + O(1)
# Total: O(n)
```

**Fast approach (exponentiation by squaring):**
```python
def power_fast(x, n):
    if n == 0:
        return 1
    if n % 2 == 0:
        half = power_fast(x, n // 2)  # T(n/2)
        return half * half             # O(1)
    else:
        return x * power_fast(x, n - 1)
# T(n) = T(n/2) + O(1) ‚Üí O(log n)
```

**Comparison:**
| Approach | Recurrence | Complexity |
|----------|------------|------------|
| Naive | T(n) = T(n-1) + O(1) | O(n) |
| Fast | T(n) = T(n/2) + O(1) | O(log n) |

### Example 3: Tree Traversal

```python
def traverse(node):
    if node is None:
        return
    print(node.val)           # O(1)
    traverse(node.left)       # T(left subtree)
    traverse(node.right)      # T(right subtree)
```

For a **balanced** tree:
- Recurrence: T(n) = 2T(n/2) + O(1)
- Master Theorem: a=2, b=2, d=0 ‚Üí 2 > 2‚Å∞ ‚Üí Case 3
- O(n^(log‚ÇÇ2)) = **O(n)**

**Intuition:** We visit every node once, so O(n) regardless of tree shape.

### Example 4: Quick Sort

**Average case (balanced partitions):**
```python
def quicksort(arr):
    if len(arr) <= 1:
        return arr
    
    pivot = arr[len(arr) // 2]
    left = [x for x in arr if x < pivot]    # O(n)
    middle = [x for x in arr if x == pivot] # O(n)
    right = [x for x in arr if x > pivot]   # O(n)
    
    return quicksort(left) + middle + quicksort(right)
    # T(n/2) + T(n/2) + O(n) ‚Üí O(n log n)
```

**Worst case (already sorted, bad pivot):**
```
T(n) = T(n-1) + T(0) + O(n) = T(n-1) + O(n)

Tree:
T(n)      ‚Üê O(n) partition
  ‚îÇ
T(n-1)    ‚Üê O(n-1) partition
  ‚îÇ
T(n-2)    ‚Üê O(n-2) partition
  ...
Total: n + (n-1) + (n-2) + ... + 1 = O(n¬≤)
```

---

## üß† Space Complexity of Recursion

**Important:** Recursion uses **stack space** for each call!

```python
def deep_recursion(n):
    if n <= 0:
        return
    deep_recursion(n - 1)  # Each call stays on stack
```

**Space complexity:** O(n) ‚Äî n stack frames simultaneously

### Stack Overflow Risk

```python
# This will crash for large n!
def factorial(n):
    if n <= 1:
        return 1
    return n * factorial(n - 1)

factorial(10000)  # RecursionError: maximum recursion depth exceeded
```

### Tail Recursion (Optimization)

Some languages optimize **tail recursion** (where the recursive call is the last operation):

```python
# NOT tail recursive (multiplication happens after the call returns)
def factorial_not_tail(n):
    if n <= 1:
        return 1
    return n * factorial_not_tail(n - 1)

# Tail recursive (can be optimized to O(1) space)
def factorial_tail(n, accumulator=1):
    if n <= 1:
        return accumulator
    return factorial_tail(n - 1, n * accumulator)  # Last operation is the call
```

**Note:** Python doesn't optimize tail recursion, but languages like Scheme, Scala, and Haskell do.

---

## üìä Decision Tree: Analyzing Recursion

```
                  Is it recursion?
                        ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚ñº               ‚ñº               ‚ñº
   1 recursive      Multiple        Each call has
      call          branches         work O(f(n))
        ‚îÇ               ‚îÇ               ‚îÇ
        ‚ñº               ‚ñº               ‚ñº
   T(n-1)?          How many?       What size?
   T(n/2)?           (= a)           (= n/b)
        ‚îÇ               ‚îÇ               ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
              Write: T(n) = aT(n/b) + O(n^d)
                        ‚îÇ
                        ‚ñº
              Apply Master Theorem or draw tree
```

---

## ‚ö†Ô∏è Common Mistakes

### Mistake 1: Forgetting the Stack Space

```python
def recursive_sum(arr, i=0):
    if i >= len(arr):
        return 0
    return arr[i] + recursive_sum(arr, i + 1)

# Time: O(n) ‚úì
# Space: O(n) ‚úó (don't forget the call stack!)
```

### Mistake 2: Wrong Branching Factor

```python
def mystery(n):
    if n <= 1:
        return
    mystery(n // 2)
    mystery(n // 2)
    mystery(n // 2)  # 3 calls, not 2!

# T(n) = 3T(n/2) + O(1)
# NOT T(n) = 2T(n/2) + O(1)
```

### Mistake 3: Ignoring Work Done at Each Level

```python
def process(arr):
    if len(arr) <= 1:
        return arr[0] if arr else 0
    
    mid = len(arr) // 2
    
    # This loop is O(n), not O(1)!
    for x in arr:
        print(x)
    
    return process(arr[:mid]) + process(arr[mid:])

# T(n) = 2T(n/2) + O(n) ‚Üí O(n log n)
# Not O(n)!
```

---

## üé§ Interview Communication

When explaining recursive complexity:

### Step 1: Identify the Recurrence
> "Each call makes two recursive calls with half the input, and does O(n) work for the merge..."

### Step 2: Write It Down
> "So the recurrence is T(n) = 2T(n/2) + O(n)."

### Step 3: Solve It
> "Using the Master Theorem with a=2, b=2, d=1, we get a = b^d, so it's Case 2: O(n log n)."

### Alternative: Use the Tree
> "Let me draw the recursion tree. We have log n levels, and each level does O(n) total work, so O(n log n)."

---

## üìù Practice: What's the Complexity?

### Problem 1

```python
def f(n):
    if n <= 1:
        return
    f(n - 1)
    f(n - 1)
```

<details>
<summary><strong>Answer</strong></summary>

**O(2‚Åø)**

Recurrence: T(n) = 2T(n-1) + O(1)

Tree: Each level doubles the calls
- Level 0: 1 call
- Level 1: 2 calls
- Level 2: 4 calls
- ...
- Level n: 2‚Åø calls

Total: O(2‚Åø)

</details>

### Problem 2

```python
def f(arr):
    if len(arr) <= 1:
        return
    mid = len(arr) // 2
    f(arr[:mid])
    f(arr[mid:])
    f(arr[:mid])
    f(arr[mid:])
```

<details>
<summary><strong>Answer</strong></summary>

**O(n¬≤)**

Recurrence: T(n) = 4T(n/2) + O(n) [for slicing]

Master Theorem: a=4, b=2, d=1
Compare: 4 vs 2¬π = 2
4 > 2, so Case 3: O(n^(log‚ÇÇ4)) = O(n¬≤)

</details>

### Problem 3

```python
def f(n):
    if n <= 1:
        return
    for i in range(n):
        print(i)
    f(n // 3)
```

<details>
<summary><strong>Answer</strong></summary>

**O(n)**

Recurrence: T(n) = T(n/3) + O(n)

Master Theorem: a=1, b=3, d=1
Compare: 1 vs 3¬π = 3
1 < 3, so Case 1: O(n^d) = O(n)

The work at the root (O(n)) dominates.

</details>

---

> **üí° Key Insight:** For recursion, always draw the tree first. It makes the pattern visible: Is work concentrated at the root? Spread evenly? Or concentrated at the leaves?

---

**Next:** [1.5 Space Complexity](./1.5-Space-Complexity.md) ‚Üí
